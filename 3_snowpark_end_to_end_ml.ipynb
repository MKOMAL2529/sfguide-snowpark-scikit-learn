{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTEBOOK 3: END TO END ML USING SNOWPARK AND SCIKIT-LEARN\n",
    "\n",
    "In this notebook we fit/train a Scikit-Learn ML pipeline that includes common feature engineering tasks such as Imputations, Scaling and One-Hot Encoding. The pipeline also includes a `RandomForestRegressor` model that will predict median house values in California. \n",
    "\n",
    "We will fit/train the pipeline using a Snowpark Python Stored Procedure (SPROC) and then log it to the Snowflake Model Registry. This example concludes by showing how a loged model/pipeline can run in a scalable fashion on a snowflake warehouse. \n",
    "\n",
    "We will also use Snowpark Optimized warehouse in this notebook.\n",
    "\n",
    "![Snowpark ML](images/snowflake_e2e_ml.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a session with Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowpark libs\n",
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark import functions as snow_funcs\n",
    "from snowflake.snowpark import types as snow_types\n",
    "from snowflake.snowpark import version\n",
    "\n",
    "# Snowpark ML libs for Model Registry\n",
    "from snowflake.ml.registry import Registry\n",
    "\n",
    "# Snowflake Python libs\n",
    "from snowflake.core import Root\n",
    "from snowflake.core.warehouse import Warehouse\n",
    "from snowflake.core.stage import Stage\n",
    "\n",
    "# Sickit-learn libs\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import json\n",
    "\n",
    "#Snowflake connection info\n",
    "from config import snowflake_conn_prop\n",
    "\n",
    "print(f'Snowpark version : {version.VERSION}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session.builder.configs(snowflake_conn_prop).create()\n",
    "#root = Root(session)\n",
    "\n",
    "print(session.sql('select current_warehouse(), current_database(), current_schema()').collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Snowflake stage\n",
    "\n",
    "In order to create a permanent Stored Procedure, model training in Snowflake, we need a Snowflake stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Root(session)\n",
    "\n",
    "stage_name=\"qs_sklearn_stage\"\n",
    "\n",
    "code_stage = Stage(\n",
    "  name=stage_name\n",
    ")\n",
    "\n",
    "code_stage = root.databases[snowflake_conn_prop['database']].schemas[snowflake_conn_prop['schema']].stages.create(code_stage, mode='or_replace')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stored Proc fits the pipeline and the model and then saves it in Snowflake\n",
    "\n",
    "Start by creating a training function, that creates a pipline with preprocessing of the data and then train a RandomForestRegressor model.\n",
    "\n",
    "We already saw some preprocessing steps in previous notebook but now we will create it as a function which will then be packaged as Stored procedure to run this entire python function in Snowflake\n",
    "\n",
    "We will use scickit-learn for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def fit_pipeline(X, y, cat_attribs, num_attribs):\n",
    "\n",
    "    # create a pipeline for numerical features\n",
    "    num_pipeline = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "            ('std_scaler', StandardScaler()),\n",
    "        ])\n",
    "\n",
    "    # Pipeline for categorical features\n",
    "    cat_pipeline = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    # Create the preprocessor\n",
    "    preprocessor = ColumnTransformer([\n",
    "            (\"num\", num_pipeline, num_attribs),\n",
    "            (\"cat\", cat_pipeline, cat_attribs)\n",
    "        ])\n",
    "\n",
    "    # Create the full pipeline wincluding the model training\n",
    "    full_pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)),\n",
    "        ])\n",
    "\n",
    "    # fit the preprocessing pipeline and the model together\n",
    "    full_pipeline.fit(X, y)\n",
    "\n",
    "    return full_pipeline\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the training function locally we will ned to pull back the data into a Pandas DataFrame, by using the **sample** method we can get 10% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_test = session.table(\"HOUSING_DATA\").sample(frac=0.10).to_pandas()\n",
    "pd_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the function with the sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = pd_test.loc[:, pd_test.columns != 'MEDIAN_HOUSE_VALUE']\n",
    "y = pd_test['MEDIAN_HOUSE_VALUE']\n",
    "\n",
    "test_full_pipe = fit_pipeline(X, y,  ['OCEAN_PROXIMITY'], ['LONGITUDE', 'LATITUDE', 'HOUSING_MEDIAN_AGE', 'TOTAL_ROOMS',\n",
    "       'TOTAL_BEDROOMS', 'POPULATION', 'HOUSEHOLDS', 'MEDIAN_INCOME'])\n",
    "test_full_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first prediction\n",
    "test_full_pipe.predict(X)[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now deploy the training function as a Python Stored Procedure in Snowflake, so we can run the training on Snowflake compute and do not need to move data around.\n",
    "\n",
    "We will also create a wrapper function for our training function where we can get the data and convert it to a Pandas DataFrame to be used with the training function, this is the function that will be the logic of the Stored Procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stored Procedure function\n",
    "def train_model(snf_session: Session # A stored procedure will recive a session object when executed in snowflake with the authentification done\n",
    "                , training_table: str # Table name that has the data to be used for training and test\n",
    "                , target_col: str # name of the target column\n",
    "                , m_name: str # name of the model in the Model Registry\n",
    "                , m_version: str # name of the model version\n",
    "                ) -> dict: # \n",
    "    \n",
    "    # Libraries used in the function that has not been imported as part of the python session\n",
    "    from datetime import datetime\n",
    "    import numpy as np\n",
    "\n",
    "    now = datetime.now() # Get the date and time when this is strated\n",
    "    \n",
    "    # Get the training table and split into a training and test Snowpark DataFrames\n",
    "    snowdf_train, snowdf_test = snf_session.table(training_table).random_split([0.8, 0.2], seed=82) # use seed to make the split repeatable\n",
    "\n",
    "    # Get the categorical and numeric column names\n",
    "    cat_attribs = [c.name for c in snowdf_train.schema.fields if (type(c.datatype) == snow_types.StringType) & (c.name != target_col)]\n",
    "    numeric_types = [snow_types.DecimalType, snow_types.LongType, snow_types.DoubleType, snow_types.FloatType, snow_types.IntegerType]\n",
    "    num_attribs = [c.name for c in snowdf_train.schema.fields if (type(c.datatype) in numeric_types) & (c.name != target_col)]\n",
    "\n",
    "\n",
    "    # save the train and test sets as time stamped tables in Snowflake \n",
    "    table_suffix = now.strftime(\"%Y%m%d%H%M%S\")\n",
    "    train_table_name = training_table + '_TRAIN_' + table_suffix\n",
    "    test_table_name = training_table + '_TEST_' + table_suffix\n",
    "    snowdf_train.write.mode(\"overwrite\").save_as_table(train_table_name)\n",
    "    snowdf_test.write.mode(\"overwrite\").save_as_table(test_table_name)\n",
    "\n",
    "    pd_train = snowdf_train.to_pandas()\n",
    "    \n",
    "    X_train = pd_train.loc[:, pd_train.columns != target_col]\n",
    "    y_train = pd_train[target_col]\n",
    "    \n",
    "    # Fit the model (pipeline)\n",
    "    full_pipeline = fit_pipeline(X_train, y_train, cat_attribs, num_attribs)\n",
    "\n",
    "    # predict on the test set and return the root mean squared error (RMSE)\n",
    "    pd_test = snowdf_test.to_pandas()\n",
    "    \n",
    "    X_test = pd_test.loc[:, pd_train.columns != target_col]\n",
    "    y_test = pd_test[target_col]\n",
    "    \n",
    "    # Calculate test metrics\n",
    "    housing_predictions = full_pipeline.predict(X_test)\n",
    "    lin_mse = mean_squared_error(y_test, housing_predictions)\n",
    "\n",
    "    lin_rmse = np.sqrt(lin_mse)\n",
    "\n",
    "    # Connect to Model Registry\n",
    "    snowml_registry = Registry(snf_session)\n",
    "    \n",
    "    # Log the fitted pipeline in the Model Registry\n",
    "    sklearn_mv = snowml_registry.log_model(\n",
    "                                    full_pipeline,\n",
    "                                    model_name=m_name,\n",
    "                                    version_name=m_version,\n",
    "                                    sample_input_data= X_train[:50],\n",
    "                                    conda_dependencies=[\"scikit-learn\"],\n",
    "                                    options={\"relax_version\": False},\n",
    "                                    comment = 'SKLearn pipline to predict housing prices'\n",
    "                                    ,metrics={'test_mse': lin_mse, 'test_rmse': lin_rmse}\n",
    "                        )\n",
    "\n",
    "    # Create a dict to return with test metrics and the path to the saved model pipeline\n",
    "    ret_dict = {\n",
    "        \"model_name\": sklearn_mv.model_name\n",
    "        ,\"model_version\": sklearn_mv.version_name\n",
    "        , \"fully_qualified_model_name\": sklearn_mv.fully_qualified_model_name\n",
    "        , \"train_table\": train_table_name\n",
    "        , \"test_table\": test_table_name\n",
    "    }\n",
    "    return ret_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy the train_model function to Snowflake as a Python stored procedure, Snowpark will also include the fit_pipeline function.\n",
    "\n",
    "When deploying a stored procedure we will also need to sepcify what third-party Python libraies the functions are depended on, these libraries must be avalible in the Snowflake Anaconda channel. By using the **packages** we make sure that we only include the ones needed for this stored procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_sp = snow_funcs.sproc(func=train_model, name=\"train_house_sp\" ,replace=True, is_permanent=True\n",
    "                                  , stage_location=f\"{stage_name}/sp/\", session=session\n",
    "                                  , packages=['snowflake-snowpark-python==1.23.0', 'snowflake-ml-python==1.6.4' ,'scikit-learn'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the training within the SPROC\n",
    "We will now train, test and deploy a new model in Snowflake. Everything is running on Snowflake compute without any need to move data outside of Snowflake. The resulting model will be deployed in Snowflake using Model Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_dict = json.loads(train_model_sp(session, \"HOUSING_DATA\", \"MEDIAN_HOUSE_VALUE\",  \"sklearn_housing\", \"V1\"))\n",
    "return_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the model are trained and also deployed in Snowflake using the Model Registry, we can connect to it and list the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_registry = Registry(session)\n",
    "snow_registry.show_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also list the version avalible for a specific model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_model = snow_registry.get_model(\"sklearn_housing\")\n",
    "snow_model.show_versions()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optionally : For use cases where training data size is big you can optimize execution speed of model training by using Snowpark optimized warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a snowpark optimised warehouse\n",
    "#session.sql(\"create or replace warehouse LAB_SCIKIT_SNOWPARK_WH with \\\n",
    "#                WAREHOUSE_SIZE = MEDIUM \\\n",
    "#                AUTO_SUSPEND = 60 \\\n",
    "#                WAREHOUSE_TYPE = 'SNOWPARK-OPTIMIZED' \\\n",
    "#                AUTO_RESUME = TRUE\").collect()\n",
    "\n",
    "# Create warehouse\n",
    "so_wh_name = \"LAB_SCIKIT_SNOWPARK_WH\"\n",
    "so_wh = Warehouse(\n",
    "    name=so_wh_name, \n",
    "    warehouse_size=\"MEDIUM\", \n",
    "    auto_suspend=600, \n",
    "    auto_resume='true', \n",
    "    warehouse_type='SNOWPARK-OPTIMIZED'\n",
    ")\n",
    "\n",
    "warehouses = root.warehouses\n",
    "so_wh = warehouses.create(so_wh, mode='or_replace')\n",
    "\n",
    "session.use_warehouse(so_wh_name)\n",
    "session.get_current_warehouse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the training using the new snowpark-optimized warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the training stored procedure\n",
    "return_dict = json.loads(train_model_sp(session, \"HOUSING_DATA\", \"MEDIAN_HOUSE_VALUE\",  \"sklearn_housing\", \"V2\"))\n",
    "return_dict\n",
    "# suspending the snowpark optimised warehouse\n",
    "so_wh.suspend()\n",
    "\n",
    "# using regular warehouse\n",
    "session.use_warehouse(format(snowflake_conn_prop['warehouse']))\n",
    "session.get_current_warehouse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that we now have a new version of the model registred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_model.show_versions()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the Model Version in Snowflake to make predictions over the test dataset\n",
    "\n",
    "Since the model and it's versions are loged in Snowflake using Model Registry we can use a version for making predictions on our data. We can either use the default version, V1, or the our second version. For this example we wioll use the first version which is the default.\n",
    "\n",
    "Start by checkling what functions we can call for the model version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the default version of the model\n",
    "def_mv = snow_model.default\n",
    "# Show avalible functions for the version\n",
    "def_mv.show_functions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above we can see that we have a **predict** functtion and a **explain** function, the **explain** function will give us SHAP values and the **predict** will give us the predictions.\n",
    "\n",
    "We can run the model using the test dataset that is created as part of the model training, to get the name we can execute `SHOW TABLES` to see it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowdf_test = session.table(\"HOUSING_DATA_TEST_20241108072239\") # REPLACE THE TABLE NAME WITH ONE FROM ABOVE!\n",
    "inputs = snowdf_test.drop(\"MEDIAN_HOUSE_VALUE\")\n",
    "                    \n",
    "snowdf_results = def_mv.run(inputs, function_name='predict')\n",
    "\n",
    "snowdf_results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to save the predictions in Snowflake we can do `snowdf_results.write.save_as_table(\"my_table_name\", mode='overwrite')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowpark_scikit_learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
